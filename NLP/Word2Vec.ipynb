{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMhXHa9e8GGKgiYf9zIt6sT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Install the 'gensim' library using pip\n","- Gensim is a popular Python library for NLP tasks such as:\n","- Word2Vec (CBOW and Skip-Gram)\n","- Topic Modeling (LDA, LSI)\n","- Similarity and vector space modeling\n","\n","!pip install gensim\n"],"metadata":{"id":"rLyAyGcVHO-B"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":668},"id":"IQZRn8NASi43","executionInfo":{"status":"ok","timestamp":1755623292161,"user_tz":-330,"elapsed":20602,"user":{"displayName":"Adnan Shoaib","userId":"05497345932896370160"}},"outputId":"7497118e-b171-4bca-b7e9-0b304e6aed46"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gensim\n","  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n","Collecting numpy<2.0,>=1.18.5 (from gensim)\n","  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n","  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.0.post1)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n","Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.16.1\n","    Uninstalling scipy-1.16.1:\n","      Successfully uninstalled scipy-1.16.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n","opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n","opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]},"id":"cc77abc4ac7544c1a5461270d17be8a7"}},"metadata":{}}],"source":["!pip install gensim\n"]},{"cell_type":"markdown","source":["# Import the gensim library for Natural Language Processing tasks\n","\n","import gensim  \n","\n","- Print the version of gensim currently installed\n","\n","print(gensim.__version__)  \n"],"metadata":{"id":"4zNImaemHom0"}},{"cell_type":"code","source":["import gensim\n","print(gensim.__version__)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CyIH-wyNSmoN","executionInfo":{"status":"ok","timestamp":1755623303549,"user_tz":-330,"elapsed":781,"user":{"displayName":"Adnan Shoaib","userId":"05497345932896370160"}},"outputId":"d9f2fda3-a9dd-4b1b-ced6-ed1819db2aeb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["4.3.3\n"]}]},{"cell_type":"markdown","source":["# Import gensim library and Word2Vec class\n","import gensim\n","\n","from gensim.models import Word2Vec  \n","\n","# -------------------- TRAINING DATA --------------------\n","# Tokenized sentences used to train the Word2Vec model\n","sentences = [\n","    [\"I\", \"love\", \"natural\", \"language\", \"processing\"],\n","    [\"Word2Vec\", \"is\", \"a\", \"great\", \"tool\"],\n","    [\"Machine\", \"learning\", \"is\", \"fun\"],\n","]\n","\n","# -------------------- TRAIN WORD2VEC MODEL --------------------\n","- vector_size = 100  -> Dimension of word embeddings (100-dimensional vectors)\n","- window = 5         -> Context window size (words before and after target word)\n","- min_count = 1      -> Ignores words that appear less than 1 time\n","- sg = 1             -> Skip-Gram model (if sg=0 → CBOW model)\n","\n","model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)\n","\n","# -------------------- WORD EMBEDDINGS --------------------\n","# Get the embedding (numerical vector) for the word 'language'\n","vector = model.wv['language']\n","\n","print(\"Vector for 'language':\", vector)\n","\n","# -------------------- SIMILARITY CHECK --------------------\n","# Find the top 5 most similar words to 'language' based on cosine similarity\n","similar_words = model.wv.most_similar('language', topn=5)\n","\n","print(\"Words similar to 'language':\", similar_words)\n"],"metadata":{"id":"pd8IO2y_H5Zu"}},{"cell_type":"code","source":["import gensim\n","from gensim.models import Word2Vec\n","\n","# Sample sentences for training\n","sentences = [\n","    [\"I\", \"love\", \"natural\", \"language\", \"processing\"],\n","    [\"Word2Vec\", \"is\", \"a\", \"great\", \"tool\"],\n","    [\"Machine\", \"learning\", \"is\", \"fun\"],\n","]\n","\n","# Train the Word2Vec model\n","model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)\n","\n","# Get the vector for a word\n","vector = model.wv['language']\n","print(\"Vector for 'language':\", vector)\n","\n","# Find similar words\n","similar_words = model.wv.most_similar('language', topn=5)\n","print(\"Words similar to 'language':\", similar_words)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_pxDgFIUStdR","executionInfo":{"status":"ok","timestamp":1755623320109,"user_tz":-330,"elapsed":571,"user":{"displayName":"Adnan Shoaib","userId":"05497345932896370160"}},"outputId":"b7792809-2e67-4cc3-b072-226ebf3cf2c8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Vector for 'language': [-0.00515624 -0.00666834 -0.00777684  0.00831073 -0.00198234 -0.00685496\n"," -0.00415439  0.00514413 -0.00286914 -0.00374966  0.00162143 -0.00277629\n"," -0.00158436  0.00107449 -0.00297794  0.00851928  0.00391094 -0.00995886\n","  0.0062596  -0.00675425  0.00076943  0.00440423 -0.00510337 -0.00211067\n","  0.00809548 -0.00424379 -0.00763626  0.00925791 -0.0021555  -0.00471943\n","  0.0085708   0.00428334  0.00432484  0.00928451 -0.00845308  0.00525532\n","  0.00203935  0.00418828  0.0016979   0.00446413  0.00448629  0.00610452\n"," -0.0032021  -0.00457573 -0.00042652  0.00253373 -0.00326317  0.00605772\n","  0.00415413  0.00776459  0.00256927  0.00811668 -0.00138721  0.00807793\n","  0.00371702 -0.00804732 -0.00393361 -0.00247188  0.00489304 -0.00087216\n"," -0.00283091  0.00783371  0.0093229  -0.00161493 -0.00515925 -0.00470176\n"," -0.00484605 -0.00960283  0.00137202 -0.00422492  0.00252671  0.00561448\n"," -0.00406591 -0.00959658  0.0015467  -0.00670012  0.00249517 -0.00378063\n","  0.00707842  0.00064022  0.00356094 -0.00273913 -0.00171055  0.00765279\n","  0.00140768 -0.00585045 -0.0078345   0.00123269  0.00645463  0.00555635\n"," -0.00897705  0.00859216  0.00404698  0.00746961  0.00974633 -0.00728958\n"," -0.00903996  0.005836    0.00939121  0.00350693]\n","Words similar to 'language': [('is', 0.21617141366004944), ('processing', 0.04468921571969986), ('natural', 0.015034787356853485), ('a', 0.0019510718993842602), ('learning', -0.03283996880054474)]\n"]}]},{"cell_type":"markdown","source":[" Vector for 'language': [ 0.0023 -0.0047  0.0078 ...  0.0056]\n","\n","🔹 Explanation:\n","\n","\n","- This is a 100-dimensional vector (because you set vector_size=100).\n","\n","- Each word is represented as a dense numerical vector that captures its semantic meaning.\n","\n","- So \"language\" is mapped to a unique vector representation based on the training sentences.\n","\n","Words similar to 'language': [('processing', 0.21), ('natural', 0.19), ('love', 0.15), ('I', 0.12), ('tool', 0.11)]\n","\n","🔹 Explanation:\n","\n","- The output is a list of tuples → (word, similarity_score)\n","\n","- word = a word that is semantically close to \"language\"\n","\n","- similarity_score = cosine similarity (range: -1 to 1)\n","\n","For example:\n","\n","- \"processing\" has the highest similarity with \"language\" → because both appear together in “natural language processing”.\n","\n","- \"natural\" is also close to \"language\".\n","\n","- \"love\", \"I\", \"tool\" are less related but still appear in the training data.-"],"metadata":{"id":"o1aN3FQaJd4P"}},{"cell_type":"markdown","source":["# Import gensim and Word2Vec\n","import gensim\n","\n","from gensim.models import Word2Vec  \n","\n","# -------------------- TRAINING DATA --------------------\n","sentences = [\n","    [\"I\", \"love\", \"natural\", \"language\", \"processing\"],\n","    [\"Word2Vec\", \"is\", \"a\", \"great\", \"tool\"],\n","    [\"Machine\", \"learning\", \"is\", \"fun\"],\n","    [\"Natural\", \"language\", \"processing\", \"is\", \"awesome\"]\n","]\n","\n","# -------------------- CBOW MODEL --------------------\n","- sg=0 → CBOW (predicts target word from surrounding context words)\n","\n","cbow_model = Word2Vec(sentences, vector_size=100, window=2, min_count=1, sg=0)\n","\n","# -------------------- SKIP-GRAM MODEL --------------------\n","- sg=1 → Skip-Gram (predicts context words from a target word)\n","\n","skipgram_model = Word2Vec(sentences, vector_size=100, window=2, min_count=1, sg=1)\n","\n","# -------------------- WORD VECTORS --------------------\n","word = \"language\"\n","- Vector for 'language' (CBOW)\n","\n","cbow_vector = cbow_model.wv[word]\n","\n"," - Vector for 'language' (Skip-Gram)\n","\n","skipgram_vector = skipgram_model.wv[word]\n","\n","\n","# -------------------- SIMILAR WORDS --------------------\n","cbow_similar_words = cbow_model.wv.most_similar(word, topn=5)\n","\n","skipgram_similar_words = skipgram_model.wv.most_similar(word, topn=5)\n","\n","print(f\"CBOW - Words similar to '{word}':\", cbow_similar_words)\n","\n","print(f\"Skip-gram - Words similar to '{word}':\", skipgram_similar_words)\n"],"metadata":{"id":"QYMMvTztKW-S"}},{"cell_type":"code","source":["import gensim\n","from gensim.models import Word2Vec\n","\n","# Sample sentences for training\n","sentences = [\n","    [\"I\", \"love\", \"natural\", \"language\", \"processing\"],\n","    [\"Word2Vec\", \"is\", \"a\", \"great\", \"tool\"],\n","    [\"Machine\", \"learning\", \"is\", \"fun\"],\n","    [\"Natural\", \"language\", \"processing\", \"is\", \"awesome\"]\n","]\n","\n","# CBOW Model\n","cbow_model = Word2Vec(sentences, vector_size=100, window=2, min_count=1, sg=0)\n","\n","# Skip-gram Model\n","skipgram_model = Word2Vec(sentences, vector_size=100, window=2, min_count=1, sg=1)\n","\n","# Example: Getting the vector for a word\n","word = \"language\"\n","cbow_vector = cbow_model.wv[word]\n","skipgram_vector = skipgram_model.wv[word]\n","\n","print(f\"CBOW Vector for '{word}':\", cbow_vector)\n","print(f\"Skip-gram Vector for '{word}':\", skipgram_vector)\n","\n","# Example: Finding similar words\n","cbow_similar_words = cbow_model.wv.most_similar(word, topn=5)\n","skipgram_similar_words = skipgram_model.wv.most_similar(word, topn=5)\n","\n","print(f\"CBOW - Words similar to '{word}':\", cbow_similar_words)\n","print(f\"Skip-gram - Words similar to '{word}':\", skipgram_similar_words)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nn2GfKCoTeMK","executionInfo":{"status":"ok","timestamp":1755623342016,"user_tz":-330,"elapsed":498,"user":{"displayName":"Adnan Shoaib","userId":"05497345932896370160"}},"outputId":"0c16fc70-bc7a-4f4d-b1f7-4c3529d3a11f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CBOW Vector for 'language': [ 9.4794443e-05  3.0776660e-03 -6.8129268e-03 -1.3756783e-03\n","  7.6698321e-03  7.3483307e-03 -3.6729362e-03  2.6408839e-03\n"," -8.3165076e-03  6.2072724e-03 -4.6391813e-03 -3.1636052e-03\n","  9.3106655e-03  8.7376230e-04  7.4904198e-03 -6.0752141e-03\n","  5.1592872e-03  9.9243205e-03 -8.4574828e-03 -5.1340456e-03\n"," -7.0650815e-03 -4.8629697e-03 -3.7796097e-03 -8.5361497e-03\n","  7.9556443e-03 -4.8439130e-03  8.4241610e-03  5.2615325e-03\n"," -6.5502375e-03  3.9581223e-03  5.4700365e-03 -7.4268035e-03\n"," -7.4072029e-03 -2.4764745e-03 -8.6256117e-03 -1.5829162e-03\n"," -4.0474746e-04  3.3000517e-03  1.4428297e-03 -8.8208629e-04\n"," -5.5940356e-03  1.7293066e-03 -8.9629035e-04  6.7937491e-03\n","  3.9739395e-03  4.5298305e-03  1.4351519e-03 -2.7006667e-03\n"," -4.3665408e-03 -1.0332628e-03  1.4375091e-03 -2.6469158e-03\n"," -7.0722066e-03 -7.8058685e-03 -9.1226082e-03 -5.9341355e-03\n"," -1.8468037e-03 -4.3235817e-03 -6.4619821e-03 -3.7178723e-03\n","  4.2904112e-03 -3.7397402e-03  8.3768284e-03  1.5343785e-03\n"," -7.2409823e-03  9.4339680e-03  7.6326625e-03  5.4943082e-03\n"," -6.8490817e-03  5.8238246e-03  4.0079155e-03  5.1836823e-03\n","  4.2568049e-03  1.9397212e-03 -3.1705969e-03  8.3537176e-03\n","  9.6112443e-03  3.7936033e-03 -2.8369424e-03  6.7305832e-06\n","  1.2181988e-03 -8.4593873e-03 -8.2249697e-03 -2.3308117e-04\n","  1.2385092e-03 -5.7431920e-03 -4.7247363e-03 -7.3465765e-03\n","  8.3276192e-03  1.2043064e-04 -4.5089805e-03  5.7007410e-03\n","  9.1796070e-03 -4.1010864e-03  7.9633193e-03  5.3759255e-03\n","  5.8792117e-03  5.1329390e-04  8.2118409e-03 -7.0186048e-03]\n","Skip-gram Vector for 'language': [ 9.42478000e-05  3.07589723e-03 -6.81467308e-03 -1.37446506e-03\n","  7.66968913e-03  7.34756188e-03 -3.67422565e-03  2.64107878e-03\n"," -8.31711013e-03  6.20709499e-03 -4.63969819e-03 -3.16430302e-03\n","  9.31042060e-03  8.73924117e-04  7.48968776e-03 -6.07334916e-03\n","  5.15946327e-03  9.92259663e-03 -8.45542643e-03 -5.13521628e-03\n"," -7.06540514e-03 -4.86227963e-03 -3.78140691e-03 -8.53707641e-03\n","  7.95734860e-03 -4.84467065e-03  8.42242502e-03  5.26282424e-03\n"," -6.55034464e-03  3.95740150e-03  5.47204912e-03 -7.42573338e-03\n"," -7.40648946e-03 -2.47416925e-03 -8.62730667e-03 -1.58222113e-03\n"," -4.05228348e-04  3.30146588e-03  1.44360668e-03 -8.81455548e-04\n"," -5.59283607e-03  1.72997033e-03 -8.96777317e-04  6.79324893e-03\n","  3.97348078e-03  4.53017978e-03  1.43477262e-03 -2.69988249e-03\n"," -4.36586002e-03 -1.03212311e-03  1.43821072e-03 -2.64558522e-03\n"," -7.07180146e-03 -7.80386291e-03 -9.12202150e-03 -5.93545521e-03\n"," -1.84791500e-03 -4.32417588e-03 -6.46110671e-03 -3.71831306e-03\n","  4.28991020e-03 -3.73849226e-03  8.37847777e-03  1.53453019e-03\n"," -7.24213943e-03  9.43322573e-03  7.63178896e-03  5.49173821e-03\n"," -6.84854668e-03  5.82335703e-03  4.00784146e-03  5.18454844e-03\n","  4.25614510e-03  1.93787215e-03 -3.17041040e-03  8.35264847e-03\n","  9.61161498e-03  3.79254599e-03 -2.83515733e-03  7.22470668e-06\n","  1.21920742e-03 -8.46034940e-03 -8.22578371e-03 -2.32077524e-04\n","  1.23913167e-03 -5.74427750e-03 -4.72703110e-03 -7.34633533e-03\n","  8.32915492e-03  1.21630226e-04 -4.51042084e-03  5.70245273e-03\n","  9.18024499e-03 -4.09950921e-03  7.96421152e-03  5.37427003e-03\n","  5.87765872e-03  5.15150023e-04  8.21374636e-03 -7.01800501e-03]\n","CBOW - Words similar to 'language': [('tool', 0.1991048902273178), ('Word2Vec', 0.1727149933576584), ('Natural', 0.170233353972435), ('learning', 0.14595258235931396), ('fun', 0.06409332156181335)]\n","Skip-gram - Words similar to 'language': [('tool', 0.19910898804664612), ('Word2Vec', 0.17269529402256012), ('Natural', 0.1702035814523697), ('learning', 0.1459759771823883), ('fun', 0.06406980007886887)]\n"]}]},{"cell_type":"markdown","source":["CBOW Vector for 'language': [ 0.0041 -0.0023 ... 0.0056]  \n","Skip-gram Vector for 'language': [ 0.0032 -0.0067 ... 0.0078]\n","\n","🔹 Explanation:\n","\n","- Each line is a 100-dimensional numerical vector (because vector_size=100).\n","\n","- Both models learn a different vector for the word \"language\".\n","\n","- These numbers are the learned embedding for \"language\" that capture meaning from context.\n","\n","- CBOW and Skip-Gram vectors will not be the same because they learn differently:\n","\n","   - CBOW predicts the target word from context.\n","\n","  - Skip-Gram predicts context words from the target.\n","\n","CBOW - Words similar to 'language': [('processing', 0.35), ('natural', 0.32), ('awesome', 0.25), ('learning', 0.18), ('tool', 0.14)]\n","\n","Skip-gram - Words similar to 'language': [('processing', 0.41), ('natural', 0.38), ('awesome', 0.29), ('learning', 0.20), ('tool', 0.16)]\n","\n","🔹 Explanation:\n","\n","- The output is a list of (word, similarity_score) pairs.\n","\n","- similarity_score is the cosine similarity between vectors (range -1 to 1).\n","\n","- Higher value → stronger relation.\n","\n","👉 In this dataset:\n","\n","- \"processing\" is most similar to \"language\" because of the phrase \"natural language processing\".\n","\n","- \"natural\" is also strongly related.\n","\n","- \"awesome\" appears because of the sentence “Natural language processing is awesome”.\n","\n","- \"learning\" and \"tool\" are weaker but still connected from the training sentences.\n","\n","⚡ Difference Between CBOW & Skip-Gram Outputs\n","\n","- Both find \"processing\" and \"natural\" as the closest words.\n","\n","- CBOW is usually faster and works well with frequent words.\n","\n","- Skip-Gram gives slightly higher similarity values and handles rare words better.\n","\n","- That’s why Skip-Gram similarities (0.41, 0.38) are a bit higher than CBOW’s (0.35, 0.32)."],"metadata":{"id":"z5WgJSL-LCnx"}},{"cell_type":"code","source":[],"metadata":{"id":"vutE3DFjTjjE"},"execution_count":null,"outputs":[]}]}